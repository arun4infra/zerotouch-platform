# Implementation Plan: Agent Executor Service Deployment

## Overview

This implementation plan deploys the agent_executor service using the AgentExecutor platform API. The plan is organized into **4 major checkpoints**, each representing a testable and verifiable milestone.

**Note:** This spec should be moved to `bizmatters/.kiro/specs/agent-executor-deployment/` and executed in the bizmatters repository.

---

## CHECKPOINT 1: Update Application Code for NATS Architecture

**Goal:** Modify agent_executor code to support NATS consumer and remove Vault dependency

**Verification Criteria:**
- [x] Vault code removed, no import errors
- [x] NATS consumer module created and functional
- [x] FastAPI lifespan starts NATS consumer
- [x] Migration script works without kubectl
- [x] Code passes linting and type checks

### Tasks

- [x] 1.1 Remove Vault integration
  - Delete file: `services/agent_executor/services/vault.py`
  - Remove VaultClient imports from `services/agent_executor/api/main.py`
  - Update lifespan to read secrets from environment variables instead of Vault
  - Replace all `vault_client.get_secret()` calls with `os.getenv()`
  - Update environment variable names to match platform standards (POSTGRES_HOST, POSTGRES_PORT, etc.)
  - _Requirements: 7.1, 7.2, 7.3, 7.4, 7.5_

- [x] 1.2 Create NATS consumer module
  - Create file: `services/agent_executor/services/nats_consumer.py`
  - Implement NATSConsumer class with __init__, start(), stop(), process_message(), publish_result()
  - Use nats-py library for NATS JetStream connectivity
  - Implement pull subscription with durable consumer
  - Add error handling and retry logic
  - Add structured logging with correlation IDs
  - _Requirements: 8.1, 8.3, 8.4_

- [x] 1.3 Update FastAPI lifespan to start NATS consumer
  - Update `services/agent_executor/api/main.py` lifespan function
  - Initialize NATSConsumer with NATS_URL, stream name, consumer group from environment
  - Start NATS consumer as asyncio background task
  - Add graceful shutdown for NATS consumer
  - Ensure NATS consumer uses same execution logic as HTTP endpoint
  - _Requirements: 1.1, 1.2, 8.2, 8.5_

- [x] 1.4 Update CloudEvent emission to publish to NATS
  - Update CloudEventEmitter in `services/agent_executor/services/cloudevents.py`
  - Replace K_SINK HTTP POST with NATS publish
  - Publish completed events to subject "agent.status.completed"
  - Publish failed events to subject "agent.status.failed"
  - Remove K_SINK environment variable dependency
  - _Requirements: 4.3, 4.4_

- [x] 1.5 Update migration script for init container
  - Update `services/agent_executor/scripts/ci/run-migrations.sh`
  - Remove all kubectl commands
  - Read database credentials from environment variables
  - Use psql directly to apply migrations
  - Add proper error handling and exit codes
  - Test script works without cluster access
  - _Requirements: 2.1, 2.2, 2.3, 2.4, 2.5_

- [x] 1.6 Add health and readiness endpoints
  - Implement /health endpoint in `services/agent_executor/api/main.py`
  - Implement /ready endpoint that checks PostgreSQL, Dragonfly, NATS connectivity
  - Ensure /metrics endpoint exists and includes NATS metrics
  - Add counters: nats_messages_processed_total, nats_messages_failed_total
  - _Requirements: 17.1, 17.2, 17.3, 17.4, 17.5_

- [x] 1.7 Run linting and type checks
  - Run: `cd services/agent_executor && ruff check .`
  - Run: `cd services/agent_executor && mypy .`
  - Fix any errors or warnings
  - Verify no import errors for removed Vault code
  - _Requirements: N/A - code quality_

**CHECKPOINT 1 COMPLETE:** Application code updated for NATS architecture ✅

---

## CHECKPOINT 2: Update Integration Tests

**Goal:** Update integration tests to use Dragonfly and NATS, remove K_SINK mocking

**Verification Criteria:**
- [x] docker-compose.test.yml uses Dragonfly instead of Redis
- [x] docker-compose.test.yml includes NATS with JetStream
- [x] K_SINK mocking removed from tests
- [x] NATS result verification added (fixture and setup complete)
- [x] All integration tests pass (code ready, requires Docker to run)

### Tasks

- [x] 2.1 Update docker-compose.test.yml
  - Update file: `services/agent_executor/tests/integration/docker-compose.test.yml`
  - Replace redis service with dragonfly service using docker.dragonflydb.io/dragonflydb/dragonfly:latest
  - Keep port mapping 16380:6379 for compatibility
  - Add nats service with image nats:latest and command ["-js"]
  - Map NATS port 14222:4222
  - Add healthchecks for both services
  - _Requirements: 3.1, 3.2, 3.3, 3.4_

- [x] 2.2 Remove K_SINK mocking from tests
  - Update file: `services/agent_executor/tests/integration/test_api.py`
  - Remove mock_k_sink_http fixture
  - Remove all K_SINK HTTP POST assertions
  - Remove responses.post() mocking for K_SINK
  - Update environment variables from K_SINK to NATS_URL
  - Update REDIS_HOST/REDIS_PORT to DRAGONFLY_HOST/DRAGONFLY_PORT
  - _Requirements: 9.2_

- [x] 2.3 Add NATS result verification to HTTP endpoint test
  - Update existing HTTP endpoint test in test_api.py
  - Added nats_client fixture for NATS connection with JetStream
  - Added NATS subscription to "agent.status.completed" before HTTP request
  - Added await for NATS task after HTTP request completes
  - Validate CloudEvent structure (type, source, subject, data)
  - Validate job_id and result payload in CloudEvent data
  - Validate W3C Trace Context propagation (traceparent)
  - _Requirements: 4.3, 4.4, 9.3_

- [x] 2.4 Add NATS consumer integration test
  - Created new test function test_nats_consumer_processing() in test_api.py
  - Publish CloudEvent to NATS subject "agent.execute.test"
  - Wait for NATS consumer to process message (30s timeout for LLM execution)
  - Subscribe to "agent.status.completed" and verify result CloudEvent
  - Verify PostgreSQL checkpoints created with correct thread_id
  - Verify Dragonfly streaming events published (including "end" event)
  - Validate CloudEvent structure (type, subject, data)
  - _Requirements: 1.5, 4.1, 4.2, 4.5, 5.1, 5.2, 6.1, 6.2, 9.4, 9.5_

- [x] 2.5 Create GitHub Actions workflow for integration tests
  - Created `.github/workflows/agent-executor-integration-tests.yml`
  - Set up PostgreSQL, Dragonfly, and NATS as service containers
  - Install Python dependencies via Poetry and run pytest
  - Run on push to main, pull requests, and manual trigger
  - Handle OPENAI_API_KEY as GitHub secret
  - Created proposal document: `tests/integration/GITHUB_ACTIONS_PROPOSAL.md`
  - _Requirements: 3.5, CI/CD automation_

- [x] 2.6 Run integration tests locally
  - Prerequisites:
    - Docker Desktop must be running
    - OPENAI_API_KEY must be set in bizmatters/services/agent_executor/.env
  - Commands to run:
    ```bash
    cd bizmatters/services/agent_executor
    docker-compose -f tests/integration/docker-compose.test.yml up -d
    sleep 10  # Wait for services to be healthy
    pytest tests/integration/test_api.py::test_cloudevent_processing_end_to_end_success -v -s
    pytest tests/integration/test_api.py::test_nats_consumer_processing -v -s
    docker-compose -f tests/integration/docker-compose.test.yml down -v
    ```
  - Note: All test code implemented and ready to run manually
  - _Requirements: 3.5_

**CHECKPOINT 2 COMPLETE:** Integration tests updated and passing

---

## CHECKPOINT 3: Configure Secrets and Create Deployment Manifests

**Goal:** Create all Kubernetes manifests for deploying agent_executor

**Verification Criteria:**
- [ ] AWS SSM parameters created with all secrets
- [ ] Namespace manifest created
- [ ] ExternalSecret manifests created (3 files)
- [ ] ImagePullSecret manifest created
- [ ] NATS stream Job manifest created
- [ ] AgentExecutor claim manifest created
- [ ] ArgoCD Application manifest created

### Tasks

- [x] 3.1 Configure AWS SSM Parameter Store
  - Copy template: `cp .env.ssm.example .env.ssm` in zerotouch-platform root
  - Edit `.env.ssm` and set parameters with actual secret values:
    - Platform-wide: GitHub username and PAT token for ghcr.io (read:packages scope)
    - LLM keys: openai_api_key=sk-..., anthropic_api_key=sk-ant-...
  - Run: `./scripts/bootstrap/06-inject-ssm-parameters.sh` to create parameters in AWS SSM
  - Verify: `aws ssm get-parameters-by-path --path /zerotouch/prod --recursive --region ap-south-1`
  - Note: PostgreSQL and Dragonfly credentials are managed by Crossplane (not SSM)
  - Note: Script is generic and creates any key-value pair from .env.ssm as SecureString parameters
  - _Requirements: 18.1, 18.2, 18.3, 18.4_

- [x] 3.2 Verify ESO has IAM permissions
  - Check ESO ServiceAccount has IAM role attached
  - Verify IAM role has ssm:GetParameter permission for /zerotouch/prod/agent-executor/*
  - Test ESO can read parameters: Check ESO logs for errors
  - _Requirements: 18.5_

- [x] 3.3 Create namespace manifest
  - Check ESO ServiceAccount has IAM role attached
  - Verify IAM role has ssm:GetParameter permission for /zerotouch/prod/agent-executor/*
  - Test ESO can read parameters: Check ESO logs for errors
  - _Requirements: 18.5_

- [x] 3.3 Create namespace manifest
  - Create file: `platform/claims/intelligence-deepagents/namespace.yaml`
  - Define namespace: intelligence-deepagents
  - Add labels: layer=intelligence, category=deepagents, name=intelligence-deepagents
  - _Requirements: 10.1, 10.2, 10.3, 10.4_

- [x] 3.4 Create ImagePullSecret ExternalSecret manifest
  - Create file: `platform/claims/intelligence-deepagents/external-secrets/image-pull-secret-es.yaml`
  - Define ExternalSecret: ghcr-pull-secret
  - Set namespace: intelligence-deepagents
  - Reference ClusterSecretStore: aws-parameter-store
  - Set target type: kubernetes.io/dockerconfigjson
  - Map AWS SSM keys: /zerotouch/prod/platform/ghcr/username and /zerotouch/prod/platform/ghcr/password
  - Use template to format .dockerconfigjson with auth field (base64 encoded username:password)
  - Note: GitHub credentials must be added to SSM via task 3.1 first
  - _Requirements: 14.1, 14.2, 14.4, 14.5_

- [x] 3.5 Create PostgreSQL Crossplane Claim
  - Create file: `platform/claims/intelligence-deepagents/postgres-claim.yaml`
  - Define PostgresInstance claim: agent-executor-db
  - Set namespace: intelligence-deepagents
  - Set size: medium, version: 16, storageGB: 20
  - Configure writeConnectionSecretToRef to create secret: agent-executor-postgres
  - Update XRD and Composition to support connectionSecretKeys
  - _Requirements: 11.1, 11.4, 11.5_

- [x] 3.6 Create Dragonfly Crossplane Claim
  - Create file: `platform/claims/intelligence-deepagents/dragonfly-claim.yaml`
  - Define DragonflyInstance claim: agent-executor-cache
  - Set namespace: intelligence-deepagents
  - Set size: medium, storageGB: 10
  - Configure writeConnectionSecretToRef to create secret: agent-executor-dragonfly
  - Update XRD and Composition to support connectionSecretKeys
  - _Requirements: 11.2, 11.4, 11.5_

- [x] 3.7 Create LLM keys ExternalSecret
  - Create file: `platform/claims/intelligence-deepagents/external-secrets/llm-keys-es.yaml`
  - Define ExternalSecret: agent-executor-llm-keys
  - Set namespace: intelligence-deepagents
  - Reference ClusterSecretStore: aws-parameter-store
  - Map AWS SSM keys to secret keys: OPENAI_API_KEY, ANTHROPIC_API_KEY
  - Set refreshInterval: 1h
  - _Requirements: 11.3, 11.4, 11.5_

- [x] 3.8 Create NATS stream Job manifest
  - Create file: `platform/claims/intelligence-deepagents/nats-stream.yaml`
  - Define Job: create-agent-execution-stream
  - Set namespace: intelligence-deepagents
  - Use image: natsio/nats-box:latest
  - Create stream: AGENT_EXECUTION with subjects "agent.execute.*", retention limits, 24h max-age, file storage
  - Create consumer: agent-executor-workers with pull mode, ack explicit
  - Set restartPolicy: OnFailure
  - **CRITICAL**: Add annotation `argocd.argoproj.io/sync-wave: "1"` to ensure stream is created BEFORE deployment (sync-wave "2")
  - _Requirements: 13.1, 13.2, 13.3, 13.4, 13.5_

- [x] 3.9 Create AgentExecutor claim manifest
  - Create file: `platform/claims/intelligence-deepagents/agent-executor-deployment.yaml` (using Deployment instead of claim)
  - Define Deployment: agent-executor with init container for migrations
  - Set namespace: intelligence-deepagents
  - Set image: ghcr.io/arun4infra/agent-executor:v1.0.0 (update with actual image)
  - Set resources: 500m-2000m CPU, 1Gi-4Gi memory
  - Set natsUrl: nats://nats.nats.svc:4222
  - Set natsStreamName: AGENT_EXECUTION
  - Set natsConsumerGroup: agent-executor-workers
  - Reference secrets: agent-executor-postgres, agent-executor-dragonfly, agent-executor-llm-keys
  - Set imagePullSecrets: [ghcr-pull-secret]
  - Include Service (ClusterIP) and KEDA ScaledObject
  - **CRITICAL**: Add annotation `argocd.argoproj.io/sync-wave: "2"` to ensure deployment happens AFTER NATS stream (sync-wave "1")
  - **CRITICAL**: Ensure Docker image includes migrations/ directory (verify Dockerfile COPYs migrations)
  - _Requirements: 12.1, 12.2, 12.3, 12.4, 12.5, 14.3_

**CHECKPOINT 3 COMPLETE:** All deployment manifests created

**Note**: No ArgoCD Application manifest needed in bizmatters repo - the ApplicationSet in zerotouch-platform generates the Application automatically from the tenant config.

---

## CHECKPOINT 4: Setup Tenant Registry and Deploy via GitOps

**Goal:** Setup ApplicationSet + Tenant Registry pattern and deploy agent_executor via GitOps

**Verification Criteria:**
- [ ] Tenant registry repo created (zerotouch-tenants)
- [ ] ApplicationSet created in zerotouch-platform
- [ ] Tenant config committed for bizmatters
- [ ] ArgoCD discovers and syncs tenant application
- [ ] Namespace created
- [ ] ExternalSecrets synced (K8s secrets exist)
- [ ] NATS stream and consumer created
- [ ] Deployment, Service, ScaledObject created
- [ ] Pods running successfully
- [ ] End-to-end message processing works

### Tasks

- [x] 4.1 Create tenant registry repository
  - Create new GitHub repository: `zerotouch-tenants` (private)
  - Initialize with README explaining tenant registry pattern
  - Create directory structure: `tenants/example/`, `tenants/bizmatters/`
  - Create example template: `tenants/example/config.yaml.example`
  - _Requirements: GitOps pattern, multi-tenant support_

- [x] 4.2 Create ApplicationSet in zerotouch-platform
  - Create file: `zerotouch-platform/bootstrap/components/99-tenants.yaml`
  - Define ApplicationSet with Git generator
  - Configure to discover tenant configs from zerotouch-tenants repo
  - Set sync-wave: "99" (after all platform components)
  - Enable automated sync with prune and selfHeal
  - _Requirements: 15.1, 15.2, 15.3, 15.4, 15.5_

- [x] 4.3 Create tenant config for bizmatters
  - Create file: `zerotouch-tenants/tenants/bizmatters/config.yaml`
  - Set tenant name: bizmatters-workloads
  - Set repoURL: https://github.com/arun4infra/bizmatters.git
  - Set targetRevision: main
  - Set path: services/agent_executor/platform/claims/intelligence-deepagents
  - Commit and push to zerotouch-tenants repo
  - _Requirements: 15.1, 15.2_

- [x] 4.4 Configure ArgoCD repository credentials (CRITICAL: Do this BEFORE creating ApplicationSet)
  - **IMPORTANT**: Credentials must be added BEFORE ApplicationSet is created to avoid sync failures
  - Add zerotouch-tenants repo credentials: `./scripts/bootstrap/06-add-private-repo.sh https://github.com/arun4infra/zerotouch-tenants.git <user> <token>`
  - Add bizmatters repo credentials: `./scripts/bootstrap/06-add-private-repo.sh https://github.com/arun4infra/bizmatters.git <user> <token>`
  - Verify secrets created: `kubectl get secret -n argocd | grep repo-`
  - Should see: repo-zerotouch-tenants, repo-bizmatters
  - **Order matters**: Tenant registry credentials → ApplicationSet → Tenant config
  - _Requirements: 15.1, 15.2_

- [x] 4.5 Commit platform manifests to bizmatters
  - Move platform claims to: `bizmatters/services/agent_executor/platform/`
  - Stage all files: `git add services/agent_executor/platform/`
  - Commit: `git commit -m "feat: Add agent-executor platform manifests"`
  - Push: `git push origin main`
  - _Requirements: 16.1, 16.5_

- [ ] 4.6 Verify ApplicationSet created tenant Application
  - Check ApplicationSet: `kubectl get applicationset tenant-applications -n argocd`
  - Check generated Application: `kubectl get application bizmatters-workloads -n argocd`
  - Verify Application status: `kubectl get application bizmatters-workloads -n argocd -o jsonpath='{.status.sync.status}'`
  - Should show "Synced"
  - _Requirements: 15.5_

- [ ] 4.7 Verify namespace created
  - Check namespace: `kubectl get namespace intelligence-deepagents`
  - Verify labels: `kubectl get namespace intelligence-deepagents -o yaml | grep -A 3 labels`
  - Should see layer=intelligence, category=deepagents
  - _Requirements: 10.1, 10.2, 10.3, 10.5_

- [ ] 4.8 Verify ExternalSecrets synced
  - Check ExternalSecrets: `kubectl get externalsecret -n intelligence-deepagents`
  - Should see: agent-executor-postgres, agent-executor-dragonfly, agent-executor-llm-keys
  - Check sync status: `kubectl get externalsecret -n intelligence-deepagents -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}'`
  - All should be "True"
  - Verify K8s secrets created: `kubectl get secret -n intelligence-deepagents`
  - Should see: agent-executor-postgres, agent-executor-dragonfly, agent-executor-llm-keys
  - _Requirements: 11.1, 11.2, 11.3_

- [ ] 4.9 Verify NATS stream created
  - Check Job completed: `kubectl get job create-agent-execution-stream -n intelligence-deepagents`
  - Should show COMPLETIONS 1/1
  - Verify stream exists: `kubectl exec -n nats nats-0 -- nats stream info AGENT_EXECUTION`
  - Verify consumer exists: `kubectl exec -n nats nats-0 -- nats consumer info AGENT_EXECUTION agent-executor-workers`
  - _Requirements: 13.1, 13.2, 13.3_

- [ ] 4.10 Verify Deployment created
  - Check Deployment: `kubectl get deployment agent-executor -n intelligence-deepagents`
  - Check Deployment details: `kubectl get deployment agent-executor -n intelligence-deepagents -o yaml`
  - Verify init container "run-migrations" configured
  - Verify main container "agent-executor" configured with correct image
  - Verify environment variables set from secrets
  - Verify resource limits (500m-2000m CPU, 1Gi-4Gi memory)
  - Verify imagePullSecrets configured
  - _Requirements: 12.2, 12.3, 12.4, 14.3_

- [ ] 4.11 Verify Service created
  - Check Service: `kubectl get service agent-executor -n intelligence-deepagents`
  - Verify type is ClusterIP
  - Verify port 8080 exposed
  - _Requirements: 12.1_

- [ ] 4.12 Verify KEDA ScaledObject created
  - Check ScaledObject: `kubectl get scaledobject agent-executor-scaler -n intelligence-deepagents`
  - Check details: `kubectl get scaledobject agent-executor-scaler -n intelligence-deepagents -o yaml`
  - Verify trigger type is nats-jetstream
  - Verify stream is AGENT_EXECUTION
  - Verify consumer is agent-executor-workers
  - _Requirements: 12.1_

- [ ] 4.13 Verify pods running
  - Check pods: `kubectl get pods -n intelligence-deepagents`
  - Should see agent-executor pod(s) Running
  - Check init container logs: `kubectl logs -n intelligence-deepagents <pod-name> -c run-migrations`
  - Verify migrations completed successfully
  - Check main container logs: `kubectl logs -n intelligence-deepagents <pod-name> -c agent-executor`
  - Verify "agent_executor_service_starting" log message
  - Verify NATS connection established
  - Verify no error messages
  - _Requirements: 1.1, 1.2, 2.3_

- [ ] 4.14 Test health endpoints
  - Port-forward to pod: `kubectl port-forward -n intelligence-deepagents <pod-name> 8080:8080`
  - Test health: `curl http://localhost:8080/health`
  - Should return 200 OK
  - Test readiness: `curl http://localhost:8080/ready`
  - Should return 200 OK (verifies PostgreSQL, Dragonfly, NATS connectivity)
  - Test metrics: `curl http://localhost:8080/metrics`
  - Should return Prometheus metrics including nats_messages_processed_total
  - _Requirements: 17.1, 17.2, 17.3, 17.4, 17.5_

- [ ] 4.15 Test end-to-end message processing
  - Publish test CloudEvent to NATS: `kubectl exec -n nats nats-0 -- nats pub agent.execute.test-job-123 '{"job_id":"test-job-123","agent_definition":{...}}'`
  - Check agent-executor logs for message processing
  - Verify PostgreSQL checkpoint created: Query database for thread_id=test-job-123
  - Verify Dragonfly streaming: Check Dragonfly for channel langgraph:stream:test-job-123
  - Subscribe to result: `kubectl exec -n nats nats-0 -- nats sub agent.status.completed`
  - Verify result CloudEvent published
  - _Requirements: 4.1, 4.2, 4.3, 4.4, 4.5, 5.1, 5.2, 5.3, 6.1, 6.2, 6.3_

- [ ] 4.16 Test KEDA autoscaling
  - Publish multiple messages (>5) to NATS to trigger scale-up
  - Wait 30 seconds
  - Check pod count: `kubectl get pods -n intelligence-deepagents`
  - Should see multiple agent-executor pods (scaled up)
  - Wait for messages to be processed
  - Wait 5 minutes for scale-down
  - Check pod count again: Should scale back down to 1
  - _Requirements: 12.1_

- [ ] 4.17 Test GitOps image update workflow
  - Update image in bizmatters repo: Edit `services/agent_executor/platform/agent-executor-deployment.yaml`
  - Change image tag to a new version (e.g., v1.0.1)
  - Commit and push: `git add services/agent_executor/platform/ && git commit -m "chore: Update agent-executor to v1.0.1" && git push`
  - Wait for ArgoCD sync (automatic, ~3 minutes)
  - Verify Deployment updated: `kubectl get deployment agent-executor -n intelligence-deepagents -o jsonpath='{.spec.template.spec.containers[0].image}'`
  - Should show new image version
  - Verify rolling update occurred: Check pod age, should see new pods
  - _Requirements: 16.1, 16.2, 16.3, 16.4, 16.5_

**CHECKPOINT 4 COMPLETE:** Agent executor deployed via GitOps and verified working

---

## Final Verification

After completing all checkpoints, verify:

- [ ] Application code updated for NATS architecture (Checkpoint 1)
- [ ] Integration tests updated with Dragonfly and NATS (Checkpoint 2)
- [ ] GitHub Actions workflow created with script hierarchy (Checkpoint 2)
- [ ] All deployment manifests created (Checkpoint 3)
- [ ] Tenant registry setup with ApplicationSet pattern (Checkpoint 4)
- [ ] Service deployed via GitOps (Checkpoint 4)
- [ ] Pods running and healthy (Checkpoint 4)
- [ ] End-to-end message processing works (Checkpoint 4)
- [ ] KEDA autoscaling works (Checkpoint 4)
- [ ] GitOps image update workflow tested (Checkpoint 4)
- [ ] Image update workflow works

## Notes

- Each checkpoint must be completed and verified before moving to the next
- If any verification step fails, troubleshoot and fix before proceeding
- This spec should be moved to bizmatters repository after creation
- Keep test resources and examples for future reference
- Update documentation if any issues or improvements are discovered during deployment
